---
title: "Stock-Market Headline Sentiment: VADER vs FinBERT"
subtitle: "One dataset · Two models · A fair comparison"
author: "Christian Parlade (Advisor: Dr. Cohen)"
date: today                                   # auto-fills on render
date-format: long
jupyter: python3

format:
  html:
    code-fold: true
    self-contained: true

execute:
  warning: false
  message: false

bibliography: references.bib
editor:
  markdown:
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)


## Introduction



Today’s markets move nearly **instantaneously** to news headlines. 
A single Bloomberg article or tweet can push a mega-cap’s share price by 
**hundreds of billions** before most humans can finish reading it. For 
data-driven traders, headline sentiment is therefore a first-line signal 
that is used to track order flow, flag risk, and populate real-time 
dashboards. The question is **which sentiment engine to trust**: a 
lightweight rule-based lexicon written in 2014, or  a 110 million-
parameter transformer fine-tuned last night?

## Research question

> *On the Financial PhraseBank corpus, does a fine‑tuned FinBERT model
> outperform the classic VADER lexicon—measured by macro‑F1 and overall
> accuracy—when classifying headlines as Positive, Neutral, or
> Negative?*

## Why it matters

- **Data‑science lens**: Quantifies the *return on GPU*: is
  domain‑specific fine‑tuning worth the extra carbon, cost, and
  complexity compared to a zero‑training heuristic [@hutto2014vader]?
- **Trading lens**: Higher‑fidelity sentiment feeds translate directly
  into fewer false long/short triggers, cleaner event studies, and more
  accurate stock basket construction.  Even a five‑point F1 lift can
  save basis points on execution.

| Component | Choice |
|-----------|--------|
| **Dataset** | *Financial PhraseBank* : “Sentences\_AllAgree” split (4,840 labelled sentences). |
| **Model A** | **VADER** rule‑based scorer with 7 k‑term lexicon [@hutto2014vader]. |
| **Model B** | **FinBERT** (`ProsusAI/finbert`) fine‑tuned for three epochs on the same training split [@yang2024finbert]. |
| **Metrics** | Macro‑F1, overall accuracy, confusion matrix; CPU/GPU runtime. |


The outcome will show whether heavy
transformers deliver practical lift over a free, no‑GPU baseline in a
realistic “small‑data” finance setting.
<!-- -->

## Methods  

### Study design  

| Step | Choice | Why it fits the problem |
|------|--------|-------------------------|
| **Dataset** | Financial PhraseBank “All Agree” (4,840 labeled headlines) | Small but clean; labels agreed on by all raters, which lowers noise |
| **Split** | 80 % train · 20 % test stratified by label | Fair hold-out and keeps the class ratio fixed |
| **Models** | 1. VADER rule system<br>2. FinBERT transformer fine-tuned | Contrasts a zero-train baseline with a modern large model |
| **Metric** | Macro-F1 (main) plus Accuracy | Macro-F1 gives each class equal weight, which matters with neutral-heavy data |
| **Validation** | 10 % of the train set for early stop and hyper-checks | Avoids peeking at the test split |

---

### Baseline: VADER  

| Item | Detail |
|------|--------|
| **Algorithm** | 7 000-term polarity lexicon plus five heuristics for negation, intensifiers, all-caps, exclamation, and the “but” rule [@hutto2014vader] |
| **Code** | `vaderSentiment 3.3.2` in Python |
| **Label rule** | `compound ≥ 0.05` → **Positive**<br>`compound ≤ −0.05` → **Negative**<br>otherwise **Neutral** |
| **Compute cost** | One pass over text, milliseconds per 1 000 headlines on CPU |
| **Rationale** | Transparent and free. Sets a floor that any heavy model must beat to justify extra cost |

---

### Large model: FinBERT fine-tune  

| Item | Detail |
|------|--------|
| **Base checkpoint** | `ProsusAI/finbert` (110 M parameters, BERT-Base size) pretrained on earnings calls, 10-Ks, and financial news [@yang2024finbert] |
| **Fine-tune goal** | Adapt to 3-class PhraseBank labels (Positive / Neutral / Negative) |
| **Loss** | Cross-entropy on softmax outputs |
| **Hyper-settings** | 3 epochs · batch 32 · learning rate 2e-5 · max sequence length 64 tokens |
| **Regularization** | Early stop on dev Macro-F1; dropout kept at 0.1 (BERT default) |
| **Why this makes sense** | Model already knows finance wording (`guidance`, `EBIT`, etc.) so only a short tune is needed. Sequence length 64 covers all headlines plus CLS/SEP tokens, avoiding wasted padding |

Mathematically, we minimise

$$
\mathcal{L}(\theta)\;=\;
-\frac{1}{N}\sum_{i=1}^{N}\;
    \sum_{c\in\{+,\;0,\;-\}}
        y_{ic}\,\log p_\theta\!\bigl(c \mid x_i\bigr)
$$

where \(y_{ic}\) is the one-hot target for class \(c\) on example \(i\), and \(p_\theta(c \mid x_i)\) is the model soft-max probability.


---

### Evaluation procedure  

1. **Freeze** the 80 / 20 split.  
2. **Train** FinBERT on the train part, using 10 % of it for dev checks.  
3. **Run** VADER straight on the untouched test headlines.  
4. **Predict** with the saved FinBERT weights on the same test headlines.  
5. **Score** both sets with macro-F1 and Accuracy, then plot confusion matrices.  
6. **Record** wall-clock timing for inference on a CPU core and the one-time GPU training cost.

This pipeline ensures both models see identical data and that no test example leaks into training.

Headline sentiment comes with tight latency budgets and mixed, domain-specific language. 
I keep a two-model setup to answer a practical trade-off: VADER gives instant, transparent scores on any CPU, 
so it sets a “good‐enough” floor that costs nothing to run. FinBERT, on the other hand, arrives with a 
finance vocabulary already baked in; a brief three-epoch tune can teach it the PhraseBank labels without large compute. 
By freezing the same 80->20 split, scoring with macro-F1 to balance the neutral skew, and timing each step, 
I can show exactly when the transformer’s extra accuracy justifies its one-time GPU bill and heavier inference.


## Analysis and Results

This section shows how the raw PhraseBank files turn into the scores
reported in the slides. All code runs in the
*03_data_analysis.qmd* and *04_results.qmd* notebooks; only the key
snippets are displayed here.

### Data exploration

We begin by loading the two pre-split CSVs:

```{python}
import pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from pathlib import Path

train = pd.read_csv(Path("data/processed/phrasebank_train.csv"))
test  = pd.read_csv(Path("data/processed/phrasebank_test.csv"))
```

#### Source & labels
The **Financial PhraseBank** “Sentences_AllAgree” subset  
(4,840 English headlines) is hand-labeled *Positive*, *Neutral*,
*Negative*. We use the text as-is—no extra cleaning beyond lower-casing
for VADER and BERT tokenization for FinBERT.

#### Class balance
Neutral makes up ≈ 46 % of both splits, the other two classes ≈ 27 %
each:

```{python}
(train["label_name"]
 .value_counts(normalize=True)
 .plot.bar(title="Class share (train set)", ylabel="proportion"))
plt.show()
```

Since a “predict Neutral” model scores 0.46 accuracy, Macro-F1 is our
primary metric.

#### Headline length
Headlines cluster at 8–16 tokens:

```{python}
train["len"] = train["sentence"].str.split().map(len)
sns.kdeplot(data=train, x="len", hue="label_name",
            common_norm=False, fill=True)
plt.xlabel("tokens"); plt.title("Length by sentiment"); plt.show()
```

`max_len 64` therefore covers 99 % of lines with no truncation.

#### Notable pattern
Many lines mix cues (“Profit beats but guidance cut”). Rules often land
Neutral; we will revisit this error mode.


### Modelling workflow

| Step | VADER (baseline) | FinBERT (fine-tuned) |
|------|------------------|----------------------|
| Token handling | whitespace + regex | WordPiece |
| Training | none | 3 epochs, batch 32, lr 2e-5 |
| Hardware | CPU only | single T4 GPU (~6 min) |
| Inference speed | 18 k/s CPU | 2 k/s CPU (batch 64) |
| Output | rule map | softmax argmax |

Both models score the same 968-row test split:

```{python}
from sklearn.metrics import f1_score, accuracy_score

def macro_f1(df):
    return f1_score(df.label_name, df.pred, average="macro")
```


### Headline-level results

| Model  | Macro-F1 | Accuracy | Test runtime |
|--------|----------|----------|--------------|
| VADER  | **0.487** | 0.575 | 28 ms CPU |
| FinBERT| **0.925** | 0.938 | 0.46 s CPU |

FinBERT lifts Macro-F1 by **+0.44** (an 89 % gain) while still
processing headlines fast enough for real-time dashboards.


### Confusion matrices

```{python}
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix; labels = ["negative","neutral","positive"]

for name,csv_file in [("VADER","vader_preds.csv"),
                      ("FinBERT","finbert_preds.csv")]:
    df = pd.read_csv(f"data/processed/{csv_file}")
    cm = confusion_matrix(df.label_name, df.pred, labels=labels)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=labels, yticklabels=labels)
    plt.title(name); plt.xlabel("pred"); plt.ylabel("true"); plt.show()
```

*Observations*  
* **VADER**: Large boundary errors—negatives and positives often collapse
  into Neutral or the wrong extreme.  
* **FinBERT**: > 92 % on the diagonal for every class; residual misses
  are genuinely low-signal headlines.

### Qualitative errors

| Headline | True | VADER | FinBERT |
|----------|------|-------|---------|
| Shares **fall** despite upbeat outlook | Neg | Neutral | **Neg** |
| Margins **compress** as input costs spike | Neg | Pos | **Neg** |
| Board proposes **higher dividend** | Pos | Neutral | **Pos** |

FinBERT handles cause/effect and finance jargon that the lexicon rules
miss.


### Discussion

FinBERT’s lift justifies the heavier footprint whenever sentiment feeds
drive trading or risk alerts. VADER still wins for ultra-lightweight
monitoring dashboards where a wrong label is cheap.

The next section lists hyper-parameters and hardware for exact
reproducibility.



## Modeling and Results

This part covers what happens *after* exploration: how each model is
built, tuned, and judged.

### Pipeline recap

1.  Load pre-split train and test CSVs  
2.  Encode text for the chosen model  
3.  Train (if needed) on train, validate on a 10 percent hold-out  
4.  Score the locked test set  
5.  Write `*_preds.csv` for later inspection

### VADER baseline

*   Pure inference, no training stage  
*   Sentiment comes from a 7 k word list plus five rule tweaks  
*   Complexity is a single pass through sentences

```{python}
import pandas as pd, numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

def vader_tag(text):
    score = sid.polarity_scores(text)["compound"]
    if score >= 0.05:   return "positive"
    if score <= -0.05:  return "negative"
    return "neutral"

test = pd.read_csv("data/processed/phrasebank_test.csv")
test["pred"] = test.sentence.map(vader_tag)
test.to_csv("data/processed/vader_preds.csv", index=False)
```

### FinBERT fine-tune

*   Base model: `ProsusAI/finbert` (110 M parameters)  
*   Train three epochs, batch 32, learning rate 2e-5  
*   Hardware: single T4 GPU, six minutes wall time



### Metric summary

| Model   | Macro-F1 | Accuracy | CPU sentences per second |
|---------|----------|----------|--------------------------|
| VADER   | 0.487 | 0.575 | 18 k |
| FinBERT | 0.925 | 0.938 | 2 k  |

Macro-F1 gives every class equal weight, stopping the neutral majority
from hiding errors.

### Where each model fails

*VADER*  
*   Overweights upbeat verbs when the context is bearish  
*   Treats mixed headlines as neutral

*FinBERT*  
*   Still confused by rare tickers or slang not in its pre-train  
*   Slightly slower; requires GPU if you expand beyond headlines

### Practical meaning

For a live headline dashboard, the cheaper VADER may be fine, but any
system that trades or alerts on sentiment will benefit from the 0.44
Macro-F1 gain. The one-time GPU fine-tune cost is roughly equal to a few
bad trades avoided.

### What the numbers reveal

The headline data tell a clear story.  VADER behaves like a cautious
news reader: it places two out of three mixed headlines in the neutral
bucket.  That looks safe but misses the market’s real sentiment and brings
Macro-F1 below fifty percent.  FinBERT, trained on filings and earnings
calls, recognizes the weight of phrases such as “cost headwinds” or
“guidance trimmed.”  It pushes those items into the negative class,
raising the diagonal scores of the confusion matrix toward one and doubling
the F1 score.

Looking across classes we find:

* **Negative**: VADER calls only one in five true negatives; FinBERT
  catches nine in ten, the largest single source of lift.
* **Neutral**: Both models agree on the easy straight-news items,
  so gains are small here.
* **Positive**: FinBERT again improves recall, helped by context
  words like “beats,” “record,” and “expands.”

The result is not just a higher score.  It changes the strategy of trading 
day-to-day: alerts fire when a company trims outlook, news screens flag the
big winners/losers, and researchers can run cleaner event studies
without fixing sentiment labels by hand. The data shows that
domain context converts a headline feed from a blunt indicator into a
decision-ready signal.



## Conclusion

### Key findings  
* A simple rule set (VADER) scores **Macro-F1 ≈ 0.49** on the
  Financial PhraseBank.  Most misses come from calling mixed or hedged
  headlines neutral.  
* A finance-tuned transformer (FinBERT) lifts performance to
  **Macro-F1 ≈ 0.93** and improves recall in every class, especially
  negative news.  
* The gain comes with moderate cost: one T4 GPU, three epochs, six
  minutes.  At inference the model runs in real time on CPU when batched.

### Implications  
* **For data scientists**: Domain pre-training plus a short
  fine-tune can double headline accuracy compared with a free heuristic.
  The result justifies the small carbon and hardware bill.  
* **For traders and risk desks**: Higher recall on bad news cuts false
  positives in long/short signals and sharpens market-mood dashboards.
  Fewer neutral “nulls” means cleaner event studies and clearer
  narrative around price moves.  
* **For engineering teams**: VADER is still useful as a fallback or
  latency-critical check, but FinBERT is ready for production once GPU
  training is complete and the model is quantized for CPU inference.


In short, the experiment shows that a light fine-tune on a domain
transformer turns headline sentiment from a rough heuristic into a
decision-grade signal, ready for use in modern trading and analytics
pipelines.


## References

- Hutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.
- Yang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097).
