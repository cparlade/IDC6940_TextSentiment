---
title: "Writing a great story for data science projects - spring 2025"
subtitle: "This is a Report Template Quarto"
author: "Students names (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.


Today’s markets move nearly **instantaneously** to news headlines. 
A single Bloomberg article or tweet can push a mega-cap’s share price by 
**hundreds of billions** before most humans can finish reading it. For 
data-driven traders, headline sentiment is therefore a first-line signal 
that is used to track order flow, flag risk, and populate real-time 
dashboards. The question is **which sentiment engine to trust**: a 
lightweight rule-based lexicon written in 2014, or  a 110 million-
parameter transformer fine-tuned last night?

## 1.1 Research question

> *On the Financial PhraseBank corpus, does a fine‑tuned FinBERT model
> outperform the classic VADER lexicon—measured by macro‑F1 and overall
> accuracy—when classifying headlines as Positive, Neutral, or
> Negative?*

## 1.2 Why it matters

- **Data‑science lens**  — Quantifies the *return on GPU*: is
  domain‑specific fine‑tuning worth the extra carbon, cost, and
  complexity compared to a zero‑training heuristic [@hutto2014vader]?
- **Trading lens**  — Higher‑fidelity sentiment feeds translate directly
  into fewer false long/short triggers, cleaner event studies, and more
  accurate thematic basket construction.  Even a five‑point F1 lift can
  save basis points on execution.

| Component | Choice |
|-----------|--------|
| **Dataset** | *Financial PhraseBank* — “Sentences\_AllAgree” split (4 ,840 labelled sentences). |
| **Model A** | **VADER** rule‑based scorer with 7 k‑term lexicon [@hutto2014vader]. |
| **Model B** | **FinBERT** (`ProsusAI/finbert`) fine‑tuned for three epochs on the same training split [@yang2024finbert]. |
| **Metrics** | Macro‑F1, overall accuracy, confusion matrix; CPU/GPU runtime. |


The outcome will show whether heavy
transformers deliver practical lift over a free, no‑GPU baseline in a
realistic “small‑data” finance setting.
<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine learning [@adams2021topology]

*This is my work and I want to add more work...*

## Methods
### Overview  
- **Goal** Compare a *rule‑based* and a *transformer* approach for classifying headline sentiment.  
- **Dataset** Financial PhraseBank (Sentences_AllAgree, N = 4 ,840).  
- **Train/Test Split** 80 % train : 20 % test (stratified).  
- **Evaluation Metrics** Macro‑F1, accuracy, confusion matrix; runtime footprint.

-   Detail the models or algorithms used.
### Baseline Model – VADER  

- **Algorithm** 7 k‑term lexicon + 5 heuristic adjustments  
  (negation, degree adverbs, all‑caps, punctuation emphasis, contrastive “but”).  
- **Implementation** `vaderSentiment 3.3.2` (Python).  
- **Label Mapping**  
  - `compound ≥ 0.05 is Positive`  
  - `compound ≤ –0.05 is Negative`  
  - else `Neutral`
- **Complexity** O(N) pass over sentences; no training phase.

### Fine‑Tuned Transformer – FinBERT  
Current proposed second model comparison below, with guidance from an LLM. Will change

- **Base Model** `ProsusAI/finbert` (110 M params, BERT‑Base).  
- **Fine‑Tuning**  
  - Loss cross‑entropy on 3‑class PhraseBank labels  
  - Hyper‑params 3 epochs · batch 32 · lr 2 × 10⁻⁵ · max_len 64  
  - Hardware single T4 GPU (~6 min).  
- **Output Mapping** soft‑max log‑probs -> `Positive/Neutral/Negative`.  
- **Regularization** early stopping on validation F1.

*Objective*  
\[
\mathcal L = -\sum_{i=1}^{N}\sum_{c\in\{+,0,-\}}
  y_{ic}\;\log p_{\theta}(c\,|\,h_i)
\]
-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
