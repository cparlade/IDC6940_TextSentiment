[
  {
    "objectID": "slides.html#agenda",
    "href": "slides.html#agenda",
    "title": "Stock Market Headline News Sentiment",
    "section": "0) Agenda",
    "text": "0) Agenda\n\nProblem & hypothesis\nData (Financial PhraseBank)\nBaseline vs. Transformer\nTrain/test & metrics\nResults -&gt; where each model wins/loses\nCost, limits, next steps"
  },
  {
    "objectID": "slides.html#problem",
    "href": "slides.html#problem",
    "title": "Stock Market Headline News Sentiment",
    "section": "1) Problem",
    "text": "1) Problem\n\nQuestion: Do short financial headlines carry sentiment we can reliably classify?\nWhy care: quick market mood checks, features for trading models, sanity-checks on dashboards.\nPlan: same dataset, same split to compare a rule-based baseline vs a transformer.\nHypothesis: A finance-tuned transformer (FinBERT) will beat a rule-based baseline (VADER), especially on hedged or mixed-polarity headlines."
  },
  {
    "objectID": "slides.html#data-financial-phrasebank",
    "href": "slides.html#data-financial-phrasebank",
    "title": "Stock Market Headline News Sentiment",
    "section": "2) Data (Financial PhraseBank)",
    "text": "2) Data (Financial PhraseBank)\n\n~4.8k single-sentence finance statements\n\nLabels: negative / neutral / positive\nPre-split 80/20 stratified (same split for both models)\n\n\n\nTrain/Test CSVs present: True\nTrain size: 1807  Test size: 452"
  },
  {
    "objectID": "slides.html#eda-class-balance",
    "href": "slides.html#eda-class-balance",
    "title": "Stock Market Headline News Sentiment",
    "section": "3) EDA: class balance",
    "text": "3) EDA: class balance\n\n\nNeutral dominates -&gt; strong prior; models must beat “predict neutral.”"
  },
  {
    "objectID": "slides.html#eda-length-sanity-check",
    "href": "slides.html#eda-length-sanity-check",
    "title": "Stock Market Headline News Sentiment",
    "section": "4) EDA: length sanity check",
    "text": "4) EDA: length sanity check"
  },
  {
    "objectID": "slides.html#models",
    "href": "slides.html#models",
    "title": "Stock Market Headline News Sentiment",
    "section": "5) Models",
    "text": "5) Models\n\nVADER (baseline)\n\nLexicon + rules (negation, intensity, punctuation, ALL-CAPS)\nFast & transparent; tends to over-predict neutral\n\nFinBERT (fine-tuned)\n\nBERT pretrained on finance text\n3-epoch fine-tune; better at context and hedged language"
  },
  {
    "objectID": "slides.html#traintest-process",
    "href": "slides.html#traintest-process",
    "title": "Stock Market Headline News Sentiment",
    "section": "6) Train/test process",
    "text": "6) Train/test process\n\nKeep exact same 80/20 stratified split\nMetrics: Macro-F1 (primary) + Accuracy\nSame split, same preprocessing across models; results computed on the same test set."
  },
  {
    "objectID": "slides.html#results",
    "href": "slides.html#results",
    "title": "Stock Market Headline News Sentiment",
    "section": "7) Results",
    "text": "7) Results\n\n\n\n\n\n\n\n\n\nModel\nMacro‑F1\nAccuracy\n\n\n\n\n0\nVADER\n0.487\n0.575\n\n\n1\nFinBERT\n0.925\n0.938\n\n\n\n\n\n\n\n\nFinBERT dominates: Macro-F1 0.925 vs VADER 0.487 | Accuracy 0.938 vs 0.575.\nMacro-F1 treats neg/neu/pos equally which means neutral-heavy models can’t coast.\nLift is concentrated in neg/pos recovery from “neutral.”\nLift comes from context + domain pretraining (hedged language, finance terms)."
  },
  {
    "objectID": "slides.html#confusion-matrices",
    "href": "slides.html#confusion-matrices",
    "title": "Stock Market Headline News Sentiment",
    "section": "8) Confusion matrices",
    "text": "8) Confusion matrices"
  },
  {
    "objectID": "slides.html#confusion-matrices-continued",
    "href": "slides.html#confusion-matrices-continued",
    "title": "Stock Market Headline News Sentiment",
    "section": "Confusion Matrices continued",
    "text": "Confusion Matrices continued\n\nVADER: large boundary errors - negatives and positives frequently land in neutral or even opposite classes.\nFinBERT: diagonals near 0.92-0.95 across all classes; residual mistakes are weak-polarity headlines.\nBiggest win: FinBERT disambiguates “mixed” headlines (e.g., “revenue up, margins compress”).\nFinBERT turns many VADER false neutrals into correct neg/pos, especially on hedged phrasing (‘despite’, ‘amid’)."
  },
  {
    "objectID": "slides.html#qualitative-error-analysis",
    "href": "slides.html#qualitative-error-analysis",
    "title": "Stock Market Headline News Sentiment",
    "section": "9) Qualitative error analysis",
    "text": "9) Qualitative error analysis\n\nMixed polarity: “Shares fall despite upbeat outlook.”\n\nVADER: often neutral (cancels signals)\nFinBERT: negative (weights market reaction)\n\nHedging / finance jargon: “Guidance trimmed amid macro uncertainty.”\n\nVADER: may drift neutral/positive (focus on “guidance”)\nFinBERT: negative (learned ‘trimmed/uncertainty’ patterns)\n\nForward-looking phrase: “Expects cost headwinds to ease.”\n\nBoth can wobble; FinBERT more likely positive given “ease”."
  },
  {
    "objectID": "slides.html#cost-practicality",
    "href": "slides.html#cost-practicality",
    "title": "Stock Market Headline News Sentiment",
    "section": "10) Cost & practicality",
    "text": "10) Cost & practicality\n\nVADER\n\nPros: transparent, instant on CPU, zero training.\nCons: brittle with hedging, negation scope, and domain phrasing.\n\nFinBERT (fine-tuned)\n\nPros: state-of-the-art accuracy, robust to phrasing.\nCons: GPU helpful for training; slightly heavier at inference (batch it).\n\nRecommendation\n\nUse VADER for lightweight, glanceable dashboards.\nUse FinBERT for reporting, alerts, or trading features where mistakes cost you."
  },
  {
    "objectID": "slides.html#limits-next-steps",
    "href": "slides.html#limits-next-steps",
    "title": "Stock Market Headline News Sentiment",
    "section": "11) Limits & next steps",
    "text": "11) Limits & next steps\n\nLimits\n\nShort headlines only; English; dataset is older and domain drift likely.\nNo aspect sentiment (e.g., revenue vs. costs vs. guidance).\n\nNext steps\n\nDomain adapt on recent 10-K MD&A / earnings call snippets.\nAdd aspect tags like “rating cut vs. price target raised.”\nCalibrate thresholds for “uncertain/ambiguous” to avoid false confidence."
  },
  {
    "objectID": "slides.html#takeaways",
    "href": "slides.html#takeaways",
    "title": "Stock Market Headline News Sentiment",
    "section": "12) Takeaways",
    "text": "12) Takeaways\n\nTransformer lift is real: FinBERT turns many VADER “neutral”s into correct neg/pos, driving a big Macro-F1 gain.\nMetric choice matters: Use Macro-F1 as the KPI so the model can’t coast by predicting neutral in an imbalanced 3-class setup.\nWhen to choose which: VADER for speed/transparency and lightweight dashboards; FinBERT for decisioning/alerts where boundary flips cost you.\nOperationalization is straightforward: Train once; do batched CPU inference, cache predictions, and monitor class drift.\nNext step with impact: Domain-adapt on recent filings/calls and add aspect sentiment (revenue/costs/guidance) to make signals actionable."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Stock Market Headline News Sentiment",
    "section": "References",
    "text": "References\n\nHutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.\nYang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097).\nLoughran & McDonald (2011). When Is a Liability Not a Liability? Journal of Finance (finance sentiment lexicon context)."
  },
  {
    "objectID": "03_data_analysis.html",
    "href": "03_data_analysis.html",
    "title": "3 Data & EDA",
    "section": "",
    "text": "python: /home/codespace/.python/current/bin/python\n\n\n\nraw = pd.read_csv(\"data/raw/phrasebank_sentences.csv\")\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nMerrill Lynch analyst Campbell Morgan upgraded...\n2\npositive\n\n\n1\nEriikka S+Âderstr+Âm has previously held sever...\n1\nneutral\n\n\n2\nThe webcast may be followed online on the comp...\n1\nneutral\n\n\n3\nTypical end-uses include roof structures , flo...\n1\nneutral\n\n\n4\nThe sale will be finalized in September or Oct...\n1\nneutral\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1807,\n 452,\n label_name\n neutral     0.614\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.5\n10.2\n22.0\n5\n56\n\n\nneutral\n1109\n21.0\n9.6\n20.0\n2\n81\n\n\npositive\n456\n24.8\n10.6\n23.0\n7\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "03_data_analysis.html#load-financial-phrasebank",
    "href": "03_data_analysis.html#load-financial-phrasebank",
    "title": "3 Data & EDA",
    "section": "",
    "text": "python: /home/codespace/.python/current/bin/python\n\n\n\nraw = pd.read_csv(\"data/raw/phrasebank_sentences.csv\")\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nMerrill Lynch analyst Campbell Morgan upgraded...\n2\npositive\n\n\n1\nEriikka S+Âderstr+Âm has previously held sever...\n1\nneutral\n\n\n2\nThe webcast may be followed online on the comp...\n1\nneutral\n\n\n3\nTypical end-uses include roof structures , flo...\n1\nneutral\n\n\n4\nThe sale will be finalized in September or Oct...\n1\nneutral\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1807,\n 452,\n label_name\n neutral     0.614\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.5\n10.2\n22.0\n5\n56\n\n\nneutral\n1109\n21.0\n9.6\n20.0\n2\n81\n\n\npositive\n456\n24.8\n10.6\n23.0\n7\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stock Market Headline News Sentiment",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Stock Market Headline News Sentiment",
    "section": "Introduction",
    "text": "Introduction\nToday’s markets move nearly instantaneously to news headlines. A single Bloomberg article or tweet can push a mega-cap’s share price by hundreds of billions before most humans can finish reading it. For data-driven traders, headline sentiment is therefore a first-line signal that is used to track order flow, flag risk, and populate real-time dashboards. The question is which sentiment engine to trust: a lightweight rule-based lexicon written in 2014, or a 110 million- parameter transformer fine-tuned last night?"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Stock Market Headline News Sentiment",
    "section": "1.1 Research question",
    "text": "1.1 Research question\n\nOn the Financial PhraseBank corpus, does a fine‑tuned FinBERT model outperform the classic VADER lexicon—measured by macro‑F1 and overall accuracy—when classifying headlines as Positive, Neutral, or Negative?"
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "Stock Market Headline News Sentiment",
    "section": "1.2 Why it matters",
    "text": "1.2 Why it matters\n\nData‑science lens  — Quantifies the return on GPU: is domain‑specific fine‑tuning worth the extra carbon, cost, and complexity compared to a zero‑training heuristic (Hutto and Gilbert 2014)?\nTrading lens  — Higher‑fidelity sentiment feeds translate directly into fewer false long/short triggers, cleaner event studies, and more accurate thematic basket construction. Even a five‑point F1 lift can save basis points on execution.\n\n\n\n\n\n\n\n\nComponent\nChoice\n\n\n\n\nDataset\nFinancial PhraseBank — “Sentences_AllAgree” split (4 ,840 labelled sentences).\n\n\nModel A\nVADER rule‑based scorer with 7 k‑term lexicon (Hutto and Gilbert 2014).\n\n\nModel B\nFinBERT (ProsusAI/finbert) fine‑tuned for three epochs on the same training split (Yang, Uy, and Huang 2020).\n\n\nMetrics\nMacro‑F1, overall accuracy, confusion matrix; CPU/GPU runtime.\n\n\n\nThe outcome will show whether heavy transformers deliver practical lift over a free, no‑GPU baseline in a realistic “small‑data” finance setting."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Stock Market Headline News Sentiment",
    "section": "Methods",
    "text": "Methods\n### Overview\n- Goal Compare a rule‑based and a transformer approach for classifying headline sentiment.\n- Dataset Financial PhraseBank (Sentences_AllAgree, N = 4 ,840).\n- Train/Test Split 80 % train : 20 % test (stratified).\n- Evaluation Metrics Macro‑F1, accuracy, confusion matrix; runtime footprint.\n\nDetail the models or algorithms used. ### Baseline Model – VADER\nAlgorithm 7 k‑term lexicon + 5 heuristic adjustments\n(negation, degree adverbs, all‑caps, punctuation emphasis, contrastive “but”).\n\nImplementation vaderSentiment 3.3.2 (Python).\n\nLabel Mapping\n\ncompound ≥ 0.05 is Positive\n\ncompound ≤ –0.05 is Negative\n\nelse Neutral\n\nComplexity O(N) pass over sentences; no training phase.\n\n\nFine‑Tuned Transformer – FinBERT\nCurrent proposed second model comparison below, with guidance from an LLM. Will change\n\nBase Model ProsusAI/finbert (110 M params, BERT‑Base).\n\nFine‑Tuning\n\nLoss cross‑entropy on 3‑class PhraseBank labels\n\nHyper‑params 3 epochs · batch 32 · lr 2 × 10⁻⁵ · max_len 64\n\nHardware single T4 GPU (~6 min).\n\n\nOutput Mapping soft‑max log‑probs -&gt; Positive/Neutral/Negative.\n\nRegularization early stopping on validation F1.\n\nObjective\n[ L = -{i=1}^{N}{c{+,0,-}} y_{ic};p_{}(c,|,h_i) ] - Justify your choices based on the problem and data."
  },
  {
    "objectID": "index.html#analysis-and-results",
    "href": "index.html#analysis-and-results",
    "title": "Stock Market Headline News Sentiment",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData Exploration and Visualization\n\n\nModeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals.\n\n\n\nConclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Stock Market Headline News Sentiment",
    "section": "References",
    "text": "References\n\nHutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.\nYang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097).\nLoughran & McDonald (2011). When Is a Liability Not a Liability? Journal of Finance (finance sentiment lexicon context)."
  },
  {
    "objectID": "04_results.html",
    "href": "04_results.html",
    "title": "Results & Discussion",
    "section": "",
    "text": "from pathlib import Path\nimport pandas as pd, numpy as np\n\nPROC = Path(\"data/processed\")\ntest_path = PROC/\"phrasebank_test.csv\"\nassert test_path.exists(), \"Run 03_data_analysis.qmd first to create data/processed/phrasebank_test.csv\"\n\n# Official test split (ground truth)\ntest = pd.read_csv(test_path).reset_index().rename(columns={\"index\":\"id\"})\nlabels = [\"negative\",\"neutral\",\"positive\"]\n\ndef load_and_align(pred_path, test_df=test):\n    df = pd.read_csv(pred_path)\n    if \"pred\" not in df.columns:\n        raise ValueError(f\"{pred_path} must contain a 'pred' column.\")\n\n    # normalize label text\n    for col in (\"pred\",\"label_name\"):\n        if col in df.columns and df[col].dtype == object:\n            df[col] = df[col].str.lower().str.strip()\n\n    # align by id -&gt; sentence -&gt; order\n    if \"id\" in df.columns:\n        merged = test_df.merge(df[[\"id\",\"pred\"]], on=\"id\", how=\"left\")\n    elif \"sentence\" in df.columns:\n        merged = test_df.merge(df[[\"sentence\",\"pred\"]], on=\"sentence\", how=\"left\")\n    else:\n        if len(df) != len(test_df):\n            raise ValueError(f\"Length mismatch: {pred_path} has {len(df)} rows; test has {len(test_df)}\")\n        merged = test_df.copy()\n        merged[\"pred\"] = df[\"pred\"].values\n\n    if merged[\"pred\"].isna().any():\n        missing = merged[\"pred\"].isna().sum()\n        raise ValueError(f\"{pred_path}: {missing} predictions missing after alignment\")\n\n    # keep only allowed labels\n    bad = ~merged[\"pred\"].isin(labels)\n    if bad.any():\n        raise ValueError(f\"{pred_path}: found labels outside {labels}: {sorted(merged.loc[bad,'pred'].unique())}\")\n    return merged\n\nvader = load_and_align(PROC/\"vader_preds.csv\")\nfinb  = load_and_align(PROC/\"finbert_preds.csv\")\n\ny_true = test[\"label_name\"].str.lower()\ny_vdr  = vader[\"pred\"]\ny_fnb  = finb[\"pred\"]\n\nlen(test), y_true.value_counts()\n\n(452,\n label_name\n neutral     277\n positive    114\n negative     61\n Name: count, dtype: int64)\n\n\n\nfrom sklearn.metrics import f1_score, accuracy_score\nimport pandas as pd\n\nsummary = pd.DataFrame({\n    \"Model\": [\"VADER\", \"FinBERT\"],\n    \"Macro‑F1\": [\n        f1_score(y_true, y_vdr, average=\"macro\"),\n        f1_score(y_true, y_fnb, average=\"macro\"),\n    ],\n    \"Accuracy\": [\n        accuracy_score(y_true, y_vdr),\n        accuracy_score(y_true, y_fnb),\n    ],\n}).round(3)\nsummary\n\n\n\n\n\n\n\n\nModel\nMacro‑F1\nAccuracy\n\n\n\n\n0\nVADER\n0.487\n0.575\n\n\n1\nFinBERT\n0.925\n0.938\n\n\n\n\n\n\n\n\nimport seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nfig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True)\nfor ax, name, y_pred in zip(axes, [\"VADER\",\"FinBERT\"], [y_vdr, y_fnb]):\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_title(name); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\nplt.show()\n\n\n\n\nConfusion matrices (rows = true, cols = pred)\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True)\nfor ax, name, y_pred in zip(axes, [\"VADER\",\"FinBERT\"], [y_vdr, y_fnb]):\n    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=\"true\")\n    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", vmin=0, vmax=1,\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_title(name + \" (normalized)\"); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\nplt.show()\n\n\n\n\nRow-normalized confusion matrices\n\n\n\n\n\nfrom pathlib import Path\nRESULTS = Path(\"results\"); RESULTS.mkdir(exist_ok=True)\nFIGS = Path(\"figs\"); FIGS.mkdir(exist_ok=True)\n\nsummary.to_csv(RESULTS/\"summary_metrics.csv\", index=False)\n\ndef save_cm(y_pred, path, title):\n    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=\"true\")\n    import seaborn as sns, matplotlib.pyplot as plt\n    plt.figure(figsize=(4.2,3.4))\n    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=labels, yticklabels=labels)\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n\nsave_cm(y_vdr, FIGS/\"cm_vader.png\",   \"VADER — Confusion Matrix\")\nsave_cm(y_fnb, FIGS/\"cm_finbert.png\", \"FinBERT — Confusion Matrix\")\n\n\"Saved results/summary_metrics.csv, figs/cm_vader.png, figs/cm_finbert.png\"\n\n'Saved results/summary_metrics.csv, figs/cm_vader.png, figs/cm_finbert.png'"
  }
]