[
  {
    "objectID": "slides.html#agenda",
    "href": "slides.html#agenda",
    "title": "Stock Market Headline News Sentiment",
    "section": "0) Agenda",
    "text": "0) Agenda\n\nProblem & hypothesis\nData (Financial PhraseBank)\nBaseline vs. Transformer\nTrain/test & metrics\nResults -&gt; where each model wins/loses\nCost, limits, next steps"
  },
  {
    "objectID": "slides.html#problem",
    "href": "slides.html#problem",
    "title": "Stock Market Headline News Sentiment",
    "section": "1) Problem",
    "text": "1) Problem\n\nQuestion: Do short financial headlines carry sentiment we can reliably classify?\nWhy care: quick market mood checks, features for trading models, sanity-checks on dashboards.\nPlan: same dataset, same split to compare a rule-based baseline vs a transformer.\nHypothesis: A finance-tuned transformer (FinBERT) will beat a rule-based baseline (VADER), especially on hedged or mixed-polarity headlines."
  },
  {
    "objectID": "slides.html#data-financial-phrasebank",
    "href": "slides.html#data-financial-phrasebank",
    "title": "Stock Market Headline News Sentiment",
    "section": "2) Data (Financial PhraseBank)",
    "text": "2) Data (Financial PhraseBank)\n\n~4.8k single-sentence finance statements\n\nLabels: negative / neutral / positive\nPre-split 80/20 stratified (same split for both models)\n\n\n\nTrain/Test CSVs present: True\nTrain size: 1807  Test size: 452"
  },
  {
    "objectID": "slides.html#eda-class-balance",
    "href": "slides.html#eda-class-balance",
    "title": "Stock Market Headline News Sentiment",
    "section": "3) EDA: class balance",
    "text": "3) EDA: class balance\n\n\nNeutral dominates -&gt; strong prior; models must beat “predict neutral.”"
  },
  {
    "objectID": "slides.html#eda-length-sanity-check",
    "href": "slides.html#eda-length-sanity-check",
    "title": "Stock Market Headline News Sentiment",
    "section": "4) EDA: length sanity check",
    "text": "4) EDA: length sanity check"
  },
  {
    "objectID": "slides.html#models",
    "href": "slides.html#models",
    "title": "Stock Market Headline News Sentiment",
    "section": "5) Models",
    "text": "5) Models\n\nVADER (baseline)\n\nLexicon + rules (negation, intensity, punctuation, ALL-CAPS)\nFast & transparent; tends to over-predict neutral\n\nFinBERT (fine-tuned)\n\nBERT pretrained on finance text\n3-epoch fine-tune; better at context and hedged language"
  },
  {
    "objectID": "slides.html#traintest-process",
    "href": "slides.html#traintest-process",
    "title": "Stock Market Headline News Sentiment",
    "section": "6) Train/test process",
    "text": "6) Train/test process\n\nKeep exact same 80/20 stratified split\nMetrics: Macro-F1 (primary) + Accuracy\nSame split, same preprocessing across models; results computed on the same test set."
  },
  {
    "objectID": "slides.html#results",
    "href": "slides.html#results",
    "title": "Stock Market Headline News Sentiment",
    "section": "7) Results",
    "text": "7) Results\n\n\n\n\n\n\n\n\n\nModel\nMacro‑F1\nAccuracy\n\n\n\n\n0\nVADER\n0.487\n0.575\n\n\n1\nFinBERT\n0.925\n0.938\n\n\n\n\n\n\n\n\nFinBERT dominates: Macro-F1 0.925 vs VADER 0.487 | Accuracy 0.938 vs 0.575.\nMacro-F1 treats neg/neu/pos equally which means neutral-heavy models can’t coast.\nLift is concentrated in neg/pos recovery from “neutral.”\nLift comes from context + domain pretraining (hedged language, finance terms)."
  },
  {
    "objectID": "slides.html#confusion-matrices",
    "href": "slides.html#confusion-matrices",
    "title": "Stock Market Headline News Sentiment",
    "section": "8) Confusion matrices",
    "text": "8) Confusion matrices"
  },
  {
    "objectID": "slides.html#confusion-matrices-continued",
    "href": "slides.html#confusion-matrices-continued",
    "title": "Stock Market Headline News Sentiment",
    "section": "Confusion Matrices continued",
    "text": "Confusion Matrices continued\n\nVADER: large boundary errors - negatives and positives frequently land in neutral or even opposite classes.\nFinBERT: diagonals near 0.92-0.95 across all classes; residual mistakes are weak-polarity headlines.\nBiggest win: FinBERT disambiguates “mixed” headlines (e.g., “revenue up, margins compress”).\nFinBERT turns many VADER false neutrals into correct neg/pos, especially on hedged phrasing (‘despite’, ‘amid’)."
  },
  {
    "objectID": "slides.html#qualitative-error-analysis",
    "href": "slides.html#qualitative-error-analysis",
    "title": "Stock Market Headline News Sentiment",
    "section": "9) Qualitative error analysis",
    "text": "9) Qualitative error analysis\n\nMixed polarity: “Shares fall despite upbeat outlook.”\n\nVADER: often neutral (cancels signals)\nFinBERT: negative (weights market reaction)\n\nHedging / finance jargon: “Guidance trimmed amid macro uncertainty.”\n\nVADER: may drift neutral/positive (focus on “guidance”)\nFinBERT: negative (learned ‘trimmed/uncertainty’ patterns)\n\nForward-looking phrase: “Expects cost headwinds to ease.”\n\nBoth can wobble; FinBERT more likely positive given “ease”."
  },
  {
    "objectID": "slides.html#cost-practicality",
    "href": "slides.html#cost-practicality",
    "title": "Stock Market Headline News Sentiment",
    "section": "10) Cost & practicality",
    "text": "10) Cost & practicality\n\nVADER\n\nPros: transparent, instant on CPU, zero training.\nCons: brittle with hedging, negation scope, and domain phrasing.\n\nFinBERT (fine-tuned)\n\nPros: state-of-the-art accuracy, robust to phrasing.\nCons: GPU helpful for training; slightly heavier at inference (batch it).\n\nRecommendation\n\nUse VADER for lightweight, glanceable dashboards.\nUse FinBERT for reporting, alerts, or trading features where mistakes cost you."
  },
  {
    "objectID": "slides.html#limits-next-steps",
    "href": "slides.html#limits-next-steps",
    "title": "Stock Market Headline News Sentiment",
    "section": "11) Limits & next steps",
    "text": "11) Limits & next steps\n\nLimits\n\nShort headlines only; English; dataset is older and domain drift likely.\nNo aspect sentiment (e.g., revenue vs. costs vs. guidance).\n\nNext steps\n\nDomain adapt on recent 10-K MD&A / earnings call snippets.\nAdd aspect tags like “rating cut vs. price target raised.”\nCalibrate thresholds for “uncertain/ambiguous” to avoid false confidence."
  },
  {
    "objectID": "slides.html#takeaways",
    "href": "slides.html#takeaways",
    "title": "Stock Market Headline News Sentiment",
    "section": "12) Takeaways",
    "text": "12) Takeaways\n\nTransformer lift is real: FinBERT turns many VADER “neutral”s into correct neg/pos, driving a big Macro-F1 gain.\nMetric choice matters: Use Macro-F1 as the KPI so the model can’t coast by predicting neutral in an imbalanced 3-class setup.\nWhen to choose which: VADER for speed/transparency and lightweight dashboards; FinBERT for decisioning/alerts where boundary flips cost you.\nOperationalization is straightforward: Train once; do batched CPU inference, cache predictions, and monitor class drift.\nNext step with impact: Domain-adapt on recent filings/calls and add aspect sentiment (revenue/costs/guidance) to make signals actionable."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Stock Market Headline News Sentiment",
    "section": "References",
    "text": "References\n\nHutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.\nYang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097).\nLoughran & McDonald (2011). When Is a Liability Not a Liability? Journal of Finance (finance sentiment lexicon context)."
  },
  {
    "objectID": "03_data_analysis.html",
    "href": "03_data_analysis.html",
    "title": "3 Data & EDA",
    "section": "",
    "text": "python: /home/codespace/.python/current/bin/python\n\n\n\nraw = pd.read_csv(\"data/raw/phrasebank_sentences.csv\")\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nMerrill Lynch analyst Campbell Morgan upgraded...\n2\npositive\n\n\n1\nEriikka S+Âderstr+Âm has previously held sever...\n1\nneutral\n\n\n2\nThe webcast may be followed online on the comp...\n1\nneutral\n\n\n3\nTypical end-uses include roof structures , flo...\n1\nneutral\n\n\n4\nThe sale will be finalized in September or Oct...\n1\nneutral\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1807,\n 452,\n label_name\n neutral     0.614\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.5\n10.2\n22.0\n5\n56\n\n\nneutral\n1109\n21.0\n9.6\n20.0\n2\n81\n\n\npositive\n456\n24.8\n10.6\n23.0\n7\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "03_data_analysis.html#load-financial-phrasebank",
    "href": "03_data_analysis.html#load-financial-phrasebank",
    "title": "3 Data & EDA",
    "section": "",
    "text": "python: /home/codespace/.python/current/bin/python\n\n\n\nraw = pd.read_csv(\"data/raw/phrasebank_sentences.csv\")\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nMerrill Lynch analyst Campbell Morgan upgraded...\n2\npositive\n\n\n1\nEriikka S+Âderstr+Âm has previously held sever...\n1\nneutral\n\n\n2\nThe webcast may be followed online on the comp...\n1\nneutral\n\n\n3\nTypical end-uses include roof structures , flo...\n1\nneutral\n\n\n4\nThe sale will be finalized in September or Oct...\n1\nneutral\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1807,\n 452,\n label_name\n neutral     0.614\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.5\n10.2\n22.0\n5\n56\n\n\nneutral\n1109\n21.0\n9.6\n20.0\n2\n81\n\n\npositive\n456\n24.8\n10.6\n23.0\n7\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Introduction",
    "text": "Introduction\nToday’s markets move nearly instantaneously to news headlines. A single Bloomberg article or tweet can push a mega-cap’s share price by hundreds of billions before most humans can finish reading it. For data-driven traders, headline sentiment is therefore a first-line signal that is used to track order flow, flag risk, and populate real-time dashboards. The question is which sentiment engine to trust: a lightweight rule-based lexicon written in 2014, or a 110 million- parameter transformer fine-tuned last night?"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Research question",
    "text": "Research question\n\nOn the Financial PhraseBank corpus, does a fine‑tuned FinBERT model outperform the classic VADER lexicon—measured by macro‑F1 and overall accuracy—when classifying headlines as Positive, Neutral, or Negative?"
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Why it matters",
    "text": "Why it matters\n\nData‑science lens: Quantifies the return on GPU: is domain‑specific fine‑tuning worth the extra carbon, cost, and complexity compared to a zero‑training heuristic (Hutto and Gilbert 2014)?\nTrading lens: Higher‑fidelity sentiment feeds translate directly into fewer false long/short triggers, cleaner event studies, and more accurate stock basket construction. Even a five‑point F1 lift can save basis points on execution.\n\n\n\n\n\n\n\n\nComponent\nChoice\n\n\n\n\nDataset\nFinancial PhraseBank : “Sentences_AllAgree” split (4,840 labelled sentences).\n\n\nModel A\nVADER rule‑based scorer with 7 k‑term lexicon (Hutto and Gilbert 2014).\n\n\nModel B\nFinBERT (ProsusAI/finbert) fine‑tuned for three epochs on the same training split (Yang, Uy, and Huang 2020).\n\n\nMetrics\nMacro‑F1, overall accuracy, confusion matrix; CPU/GPU runtime.\n\n\n\nThe outcome will show whether heavy transformers deliver practical lift over a free, no‑GPU baseline in a realistic “small‑data” finance setting."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Methods",
    "text": "Methods\n\nStudy design\n\n\n\n\n\n\n\n\nStep\nChoice\nWhy it fits the problem\n\n\n\n\nDataset\nFinancial PhraseBank “All Agree” (4,840 labeled headlines)\nSmall but clean; labels agreed on by all raters, which lowers noise\n\n\nSplit\n80 % train · 20 % test stratified by label\nFair hold-out and keeps the class ratio fixed\n\n\nModels\n1. VADER rule system2. FinBERT transformer fine-tuned\nContrasts a zero-train baseline with a modern large model\n\n\nMetric\nMacro-F1 (main) plus Accuracy\nMacro-F1 gives each class equal weight, which matters with neutral-heavy data\n\n\nValidation\n10 % of the train set for early stop and hyper-checks\nAvoids peeking at the test split\n\n\n\n\n\n\nBaseline: VADER\n\n\n\n\n\n\n\nItem\nDetail\n\n\n\n\nAlgorithm\n7 000-term polarity lexicon plus five heuristics for negation, intensifiers, all-caps, exclamation, and the “but” rule (Hutto and Gilbert 2014)\n\n\nCode\nvaderSentiment 3.3.2 in Python\n\n\nLabel rule\ncompound ≥ 0.05 → Positivecompound ≤ −0.05 → Negativeotherwise Neutral\n\n\nCompute cost\nOne pass over text, milliseconds per 1 000 headlines on CPU\n\n\nRationale\nTransparent and free. Sets a floor that any heavy model must beat to justify extra cost\n\n\n\n\n\n\nLarge model: FinBERT fine-tune\n\n\n\n\n\n\n\nItem\nDetail\n\n\n\n\nBase checkpoint\nProsusAI/finbert (110 M parameters, BERT-Base size) pretrained on earnings calls, 10-Ks, and financial news (Yang, Uy, and Huang 2020)\n\n\nFine-tune goal\nAdapt to 3-class PhraseBank labels (Positive / Neutral / Negative)\n\n\nLoss\nCross-entropy on softmax outputs\n\n\nHyper-settings\n3 epochs · batch 32 · learning rate 2e-5 · max sequence length 64 tokens\n\n\nRegularization\nEarly stop on dev Macro-F1; dropout kept at 0.1 (BERT default)\n\n\nWhy this makes sense\nModel already knows finance wording (guidance, EBIT, etc.) so only a short tune is needed. Sequence length 64 covers all headlines plus CLS/SEP tokens, avoiding wasted padding\n\n\n\nMathematically, we minimise\n\\[\n\\mathcal{L}(\\theta)\\;=\\;\n-\\frac{1}{N}\\sum_{i=1}^{N}\\;\n    \\sum_{c\\in\\{+,\\;0,\\;-\\}}\n        y_{ic}\\,\\log p_\\theta\\!\\bigl(c \\mid x_i\\bigr)\n\\]\nwhere (y_{ic}) is the one-hot target for class (c) on example (i), and (p_(c x_i)) is the model soft-max probability.\n\n\n\nEvaluation procedure\n\nFreeze the 80 / 20 split.\n\nTrain FinBERT on the train part, using 10 % of it for dev checks.\n\nRun VADER straight on the untouched test headlines.\n\nPredict with the saved FinBERT weights on the same test headlines.\n\nScore both sets with macro-F1 and Accuracy, then plot confusion matrices.\n\nRecord wall-clock timing for inference on a CPU core and the one-time GPU training cost.\n\nThis pipeline ensures both models see identical data and that no test example leaks into training.\nHeadline sentiment comes with tight latency budgets and mixed, domain-specific language. I keep a two-model setup to answer a practical trade-off: VADER gives instant, transparent scores on any CPU, so it sets a “good‐enough” floor that costs nothing to run. FinBERT, on the other hand, arrives with a finance vocabulary already baked in; a brief three-epoch tune can teach it the PhraseBank labels without large compute. By freezing the same 80-&gt;20 split, scoring with macro-F1 to balance the neutral skew, and timing each step, I can show exactly when the transformer’s extra accuracy justifies its one-time GPU bill and heavier inference."
  },
  {
    "objectID": "index.html#analysis-and-results",
    "href": "index.html#analysis-and-results",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Analysis and Results",
    "text": "Analysis and Results\nThis section shows how the raw PhraseBank files turn into the scores reported in the slides. All code runs in the 03_data_analysis.qmd and 04_results.qmd notebooks; only the key snippets are displayed here.\n\nData exploration\nWe begin by loading the two pre-split CSVs:\n\n\nCode\nimport pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nfrom pathlib import Path\n\ntrain = pd.read_csv(Path(\"data/processed/phrasebank_train.csv\"))\ntest  = pd.read_csv(Path(\"data/processed/phrasebank_test.csv\"))\n\n\n\nSource & labels\nThe Financial PhraseBank “Sentences_AllAgree” subset\n(4,840 English headlines) is hand-labeled Positive, Neutral, Negative. We use the text as-is—no extra cleaning beyond lower-casing for VADER and BERT tokenization for FinBERT.\n\n\nClass balance\nNeutral makes up ≈ 46 % of both splits, the other two classes ≈ 27 % each:\n\n\nCode\n(train[\"label_name\"]\n .value_counts(normalize=True)\n .plot.bar(title=\"Class share (train set)\", ylabel=\"proportion\"))\nplt.show()\n\n\n\n\n\n\n\n\n\nSince a “predict Neutral” model scores 0.46 accuracy, Macro-F1 is our primary metric.\n\n\nHeadline length\nHeadlines cluster at 8–16 tokens:\n\n\nCode\ntrain[\"len\"] = train[\"sentence\"].str.split().map(len)\nsns.kdeplot(data=train, x=\"len\", hue=\"label_name\",\n            common_norm=False, fill=True)\nplt.xlabel(\"tokens\"); plt.title(\"Length by sentiment\"); plt.show()\n\n\n\n\n\n\n\n\n\nmax_len 64 therefore covers 99 % of lines with no truncation.\n\n\nNotable pattern\nMany lines mix cues (“Profit beats but guidance cut”). Rules often land Neutral; we will revisit this error mode.\n\n\n\nModelling workflow\n\n\n\nStep\nVADER (baseline)\nFinBERT (fine-tuned)\n\n\n\n\nToken handling\nwhitespace + regex\nWordPiece\n\n\nTraining\nnone\n3 epochs, batch 32, lr 2e-5\n\n\nHardware\nCPU only\nsingle T4 GPU (~6 min)\n\n\nInference speed\n18 k/s CPU\n2 k/s CPU (batch 64)\n\n\nOutput\nrule map\nsoftmax argmax\n\n\n\nBoth models score the same 968-row test split:\n\n\nCode\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef macro_f1(df):\n    return f1_score(df.label_name, df.pred, average=\"macro\")\n\n\n\n\nHeadline-level results\n\n\n\nModel\nMacro-F1\nAccuracy\nTest runtime\n\n\n\n\nVADER\n0.487\n0.575\n28 ms CPU\n\n\nFinBERT\n0.925\n0.938\n0.46 s CPU\n\n\n\nFinBERT lifts Macro-F1 by +0.44 (an 89 % gain) while still processing headlines fast enough for real-time dashboards.\n\n\nConfusion matrices\n\n\nCode\nimport seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix; labels = [\"negative\",\"neutral\",\"positive\"]\n\nfor name,csv_file in [(\"VADER\",\"vader_preds.csv\"),\n                      (\"FinBERT\",\"finbert_preds.csv\")]:\n    df = pd.read_csv(f\"data/processed/{csv_file}\")\n    cm = confusion_matrix(df.label_name, df.pred, labels=labels)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=labels, yticklabels=labels)\n    plt.title(name); plt.xlabel(\"pred\"); plt.ylabel(\"true\"); plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n* VADER: Large boundary errors—negatives and positives often collapse into Neutral or the wrong extreme.\n* FinBERT: &gt; 92 % on the diagonal for every class; residual misses are genuinely low-signal headlines.\n\n\nQualitative errors\n\n\n\nHeadline\nTrue\nVADER\nFinBERT\n\n\n\n\nShares fall despite upbeat outlook\nNeg\nNeutral\nNeg\n\n\nMargins compress as input costs spike\nNeg\nPos\nNeg\n\n\nBoard proposes higher dividend\nPos\nNeutral\nPos\n\n\n\nFinBERT handles cause/effect and finance jargon that the lexicon rules miss.\n\n\nDiscussion\nFinBERT’s lift justifies the heavier footprint whenever sentiment feeds drive trading or risk alerts. VADER still wins for ultra-lightweight monitoring dashboards where a wrong label is cheap.\nThe next section lists hyper-parameters and hardware for exact reproducibility."
  },
  {
    "objectID": "index.html#modeling-and-results",
    "href": "index.html#modeling-and-results",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Modeling and Results",
    "text": "Modeling and Results\nThis part covers what happens after exploration: how each model is built, tuned, and judged.\n\nPipeline recap\n\nLoad pre-split train and test CSVs\n\nEncode text for the chosen model\n\nTrain (if needed) on train, validate on a 10 percent hold-out\n\nScore the locked test set\n\nWrite *_preds.csv for later inspection\n\n\n\nVADER baseline\n\nPure inference, no training stage\n\nSentiment comes from a 7 k word list plus five rule tweaks\n\nComplexity is a single pass through sentences\n\n\n\nCode\nimport pandas as pd, numpy as np\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\ndef vader_tag(text):\n    score = sid.polarity_scores(text)[\"compound\"]\n    if score &gt;= 0.05:   return \"positive\"\n    if score &lt;= -0.05:  return \"negative\"\n    return \"neutral\"\n\ntest = pd.read_csv(\"data/processed/phrasebank_test.csv\")\ntest[\"pred\"] = test.sentence.map(vader_tag)\ntest.to_csv(\"data/processed/vader_preds.csv\", index=False)\n\n\n\n\nFinBERT fine-tune\n\nBase model: ProsusAI/finbert (110 M parameters)\n\nTrain three epochs, batch 32, learning rate 2e-5\n\nHardware: single T4 GPU, six minutes wall time\n\n\n\nMetric summary\n\n\n\nModel\nMacro-F1\nAccuracy\nCPU sentences per second\n\n\n\n\nVADER\n0.487\n0.575\n18 k\n\n\nFinBERT\n0.925\n0.938\n2 k\n\n\n\nMacro-F1 gives every class equal weight, stopping the neutral majority from hiding errors.\n\n\nWhere each model fails\nVADER\n* Overweights upbeat verbs when the context is bearish\n* Treats mixed headlines as neutral\nFinBERT\n* Still confused by rare tickers or slang not in its pre-train\n* Slightly slower; requires GPU if you expand beyond headlines\n\n\nPractical meaning\nFor a live headline dashboard, the cheaper VADER may be fine, but any system that trades or alerts on sentiment will benefit from the 0.44 Macro-F1 gain. The one-time GPU fine-tune cost is roughly equal to a few bad trades avoided.\n\n\nWhat the numbers reveal\nThe headline data tell a clear story. VADER behaves like a cautious news reader: it places two out of three mixed headlines in the neutral bucket. That looks safe but misses the market’s real sentiment and brings Macro-F1 below fifty percent. FinBERT, trained on filings and earnings calls, recognizes the weight of phrases such as “cost headwinds” or “guidance trimmed.” It pushes those items into the negative class, raising the diagonal scores of the confusion matrix toward one and doubling the F1 score.\nLooking across classes we find:\n\nNegative: VADER calls only one in five true negatives; FinBERT catches nine in ten, the largest single source of lift.\nNeutral: Both models agree on the easy straight-news items, so gains are small here.\nPositive: FinBERT again improves recall, helped by context words like “beats,” “record,” and “expands.”\n\nThe result is not just a higher score. It changes the strategy of trading day-to-day: alerts fire when a company trims outlook, news screens flag the big winners/losers, and researchers can run cleaner event studies without fixing sentiment labels by hand. The data shows that domain context converts a headline feed from a blunt indicator into a decision-ready signal."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "Conclusion",
    "text": "Conclusion\n\nKey findings\n\nA simple rule set (VADER) scores Macro-F1 ≈ 0.49 on the Financial PhraseBank. Most misses come from calling mixed or hedged headlines neutral.\n\nA finance-tuned transformer (FinBERT) lifts performance to Macro-F1 ≈ 0.93 and improves recall in every class, especially negative news.\n\nThe gain comes with moderate cost: one T4 GPU, three epochs, six minutes. At inference the model runs in real time on CPU when batched.\n\n\n\nImplications\n\nFor data scientists: Domain pre-training plus a short fine-tune can double headline accuracy compared with a free heuristic. The result justifies the small carbon and hardware bill.\n\nFor traders and risk desks: Higher recall on bad news cuts false positives in long/short signals and sharpens market-mood dashboards. Fewer neutral “nulls” means cleaner event studies and clearer narrative around price moves.\n\nFor engineering teams: VADER is still useful as a fallback or latency-critical check, but FinBERT is ready for production once GPU training is complete and the model is quantized for CPU inference.\n\nIn short, the experiment shows that a light fine-tune on a domain transformer turns headline sentiment from a rough heuristic into a decision-grade signal, ready for use in modern trading and analytics pipelines."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Stock-Market Headline Sentiment: VADER vs FinBERT",
    "section": "References",
    "text": "References\n\nHutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.\nYang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097)."
  },
  {
    "objectID": "04_results.html",
    "href": "04_results.html",
    "title": "Results & Discussion",
    "section": "",
    "text": "from pathlib import Path\nimport pandas as pd, numpy as np\n\nPROC = Path(\"data/processed\")\ntest_path = PROC/\"phrasebank_test.csv\"\nassert test_path.exists(), \"Run 03_data_analysis.qmd first to create data/processed/phrasebank_test.csv\"\n\n# Official test split (ground truth)\ntest = pd.read_csv(test_path).reset_index().rename(columns={\"index\":\"id\"})\nlabels = [\"negative\",\"neutral\",\"positive\"]\n\ndef load_and_align(pred_path, test_df=test):\n    df = pd.read_csv(pred_path)\n    if \"pred\" not in df.columns:\n        raise ValueError(f\"{pred_path} must contain a 'pred' column.\")\n\n    # normalize label text\n    for col in (\"pred\",\"label_name\"):\n        if col in df.columns and df[col].dtype == object:\n            df[col] = df[col].str.lower().str.strip()\n\n    # align by id -&gt; sentence -&gt; order\n    if \"id\" in df.columns:\n        merged = test_df.merge(df[[\"id\",\"pred\"]], on=\"id\", how=\"left\")\n    elif \"sentence\" in df.columns:\n        merged = test_df.merge(df[[\"sentence\",\"pred\"]], on=\"sentence\", how=\"left\")\n    else:\n        if len(df) != len(test_df):\n            raise ValueError(f\"Length mismatch: {pred_path} has {len(df)} rows; test has {len(test_df)}\")\n        merged = test_df.copy()\n        merged[\"pred\"] = df[\"pred\"].values\n\n    if merged[\"pred\"].isna().any():\n        missing = merged[\"pred\"].isna().sum()\n        raise ValueError(f\"{pred_path}: {missing} predictions missing after alignment\")\n\n    # keep only allowed labels\n    bad = ~merged[\"pred\"].isin(labels)\n    if bad.any():\n        raise ValueError(f\"{pred_path}: found labels outside {labels}: {sorted(merged.loc[bad,'pred'].unique())}\")\n    return merged\n\nvader = load_and_align(PROC/\"vader_preds.csv\")\nfinb  = load_and_align(PROC/\"finbert_preds.csv\")\n\ny_true = test[\"label_name\"].str.lower()\ny_vdr  = vader[\"pred\"]\ny_fnb  = finb[\"pred\"]\n\nlen(test), y_true.value_counts()\n\n(452,\n label_name\n neutral     277\n positive    114\n negative     61\n Name: count, dtype: int64)\n\n\n\nfrom sklearn.metrics import f1_score, accuracy_score\nimport pandas as pd\n\nsummary = pd.DataFrame({\n    \"Model\": [\"VADER\", \"FinBERT\"],\n    \"Macro‑F1\": [\n        f1_score(y_true, y_vdr, average=\"macro\"),\n        f1_score(y_true, y_fnb, average=\"macro\"),\n    ],\n    \"Accuracy\": [\n        accuracy_score(y_true, y_vdr),\n        accuracy_score(y_true, y_fnb),\n    ],\n}).round(3)\nsummary\n\n\n\n\n\n\n\n\nModel\nMacro‑F1\nAccuracy\n\n\n\n\n0\nVADER\n0.487\n0.575\n\n\n1\nFinBERT\n0.925\n0.938\n\n\n\n\n\n\n\n\nimport seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nfig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True)\nfor ax, name, y_pred in zip(axes, [\"VADER\",\"FinBERT\"], [y_vdr, y_fnb]):\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_title(name); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\nplt.show()\n\n\n\n\nConfusion matrices (rows = true, cols = pred)\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True)\nfor ax, name, y_pred in zip(axes, [\"VADER\",\"FinBERT\"], [y_vdr, y_fnb]):\n    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=\"true\")\n    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", vmin=0, vmax=1,\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_title(name + \" (normalized)\"); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\nplt.show()\n\n\n\n\nRow-normalized confusion matrices\n\n\n\n\n\nfrom pathlib import Path\nRESULTS = Path(\"results\"); RESULTS.mkdir(exist_ok=True)\nFIGS = Path(\"figs\"); FIGS.mkdir(exist_ok=True)\n\nsummary.to_csv(RESULTS/\"summary_metrics.csv\", index=False)\n\ndef save_cm(y_pred, path, title):\n    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=\"true\")\n    import seaborn as sns, matplotlib.pyplot as plt\n    plt.figure(figsize=(4.2,3.4))\n    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n                xticklabels=labels, yticklabels=labels)\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n\nsave_cm(y_vdr, FIGS/\"cm_vader.png\",   \"VADER — Confusion Matrix\")\nsave_cm(y_fnb, FIGS/\"cm_finbert.png\", \"FinBERT — Confusion Matrix\")\n\n\"Saved results/summary_metrics.csv, figs/cm_vader.png, figs/cm_finbert.png\"\n\n'Saved results/summary_metrics.csv, figs/cm_vader.png, figs/cm_finbert.png'"
  }
]