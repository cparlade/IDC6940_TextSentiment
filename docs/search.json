[
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function (Efromovich 2008). Cite this paper (Bro and Smilde 2014). The GEE (Wang 2014). The PCA (Daffertshofer et al. 2004)*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…\n{r, warning=FALSE, echo=F, message=FALSE} # loading packages  library(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs)\n```{r, warning=FALSE, echo=F} # Load Data #kable(head(murders))\nggplot1 = murders %&gt;% ggplot(mapping = aes(x=population/10^6, y=total))\nggplot1 + geom_point(aes(col=region), size = 4) + geom_text_repel(aes(label=abb)) + scale_x_log10() + scale_y_log10() + geom_smooth(formula = “y~x”, method=lm,se = F)+ xlab(“Populations in millions (log10 scale)”) + ylab(“Total number of murders (log10 scale)”) + ggtitle(“US Gun Murders in 2010”) + scale_color_discrete(name = “Region”)+ theme_bw()\n\n## Modeling and Results\n\n-   Explain your data preprocessing and cleaning steps.\n\n-   Present your key findings in a clear and concise manner.\n\n-   Use visuals to support your claims.\n\n-   **Tell a story about what the data reveals.**\n\n```{r}"
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Headline News Sentiment & the S&P 500",
    "section": "References",
    "text": "References\n\n\n\n\nBro, Rasmus, and Age K Smilde. 2014. “Principal Component Analysis.” Analytical Methods 6 (9): 2812–31.\n\n\nDaffertshofer, Andreas, Claudine JC Lamoth, Onno G Meijer, and Peter J Beek. 2004. “PCA in Studying Coordination and Variability: A Tutorial.” Clinical Biomechanics 19 (4): 415–28.\n\n\nEfromovich, S. 2008. Nonparametric Curve Estimation: Methods, Theory, and Applications. Springer Series in Statistics. Springer New York. https://books.google.com/books?id=mdoLBwAAQBAJ.\n\n\nWang, Ming. 2014. “Generalized Estimating Equations in Longitudinal Data Analysis: A Review and Recent Developments.” Advances in Statistics 2014."
  },
  {
    "objectID": "03_data_analysis.html",
    "href": "03_data_analysis.html",
    "title": "3 Data & EDA",
    "section": "",
    "text": "datasets: 2.19.0\npython: /workspaces/IDC6940_TextSentiment/.venv/bin/python3\n\n\n\nds = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")  # single split named 'train'\nraw = pd.DataFrame(ds['train'])\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nAccording to Gran , the company has no plans t...\n1\nneutral\n\n\n1\nFor the last quarter of 2010 , Componenta 's n...\n2\npositive\n\n\n2\nIn the third quarter of 2010 , net sales incre...\n2\npositive\n\n\n3\nOperating profit rose to EUR 13.1 mn from EUR ...\n2\npositive\n\n\n4\nOperating profit totalled EUR 21.1 mn , up fro...\n2\npositive\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1811,\n 453,\n label_name\n neutral     0.615\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.3\n10.2\n22.0\n5\n54\n\n\nneutral\n1113\n21.0\n9.8\n19.0\n2\n81\n\n\npositive\n456\n24.8\n10.4\n23.0\n6\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "03_data_analysis.html#load-financial-phrasebank",
    "href": "03_data_analysis.html#load-financial-phrasebank",
    "title": "3 Data & EDA",
    "section": "",
    "text": "datasets: 2.19.0\npython: /workspaces/IDC6940_TextSentiment/.venv/bin/python3\n\n\n\nds = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")  # single split named 'train'\nraw = pd.DataFrame(ds['train'])\n# columns: 'sentence', 'label' with labels: 0=negative, 1=neutral, 2=positive\nlabel_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\nraw['label_name'] = raw['label'].map(label_map)\nraw.head()\n\n\n\n\n\n\n\n\nsentence\nlabel\nlabel_name\n\n\n\n\n0\nAccording to Gran , the company has no plans t...\n1\nneutral\n\n\n1\nFor the last quarter of 2010 , Componenta 's n...\n2\npositive\n\n\n2\nIn the third quarter of 2010 , net sales incre...\n2\npositive\n\n\n3\nOperating profit rose to EUR 13.1 mn from EUR ...\n2\npositive\n\n\n4\nOperating profit totalled EUR 21.1 mn , up fro...\n2\npositive\n\n\n\n\n\n\n\n\n# Stratified 80/20 split\ntrain_df, test_df = train_test_split(\n    raw[['sentence','label','label_name']],\n    test_size=0.20,\n    random_state=RNG_SEED,\n    stratify=raw['label'])\n\ntrain_df.to_csv(DATA_DIR/\"phrasebank_train.csv\", index=False)\ntest_df.to_csv(DATA_DIR/\"phrasebank_test.csv\", index=False)\n\nlen(train_df), len(test_df), train_df['label_name'].value_counts(normalize=True).round(3)\n\n(1811,\n 453,\n label_name\n neutral     0.615\n positive    0.252\n negative    0.134\n Name: proportion, dtype: float64)\n\n\n\nfig, ax = plt.subplots()\npd.concat({\n    \"train\": train_df['label_name'].value_counts(normalize=True).rename(\"share\"),\n    \"test\":  test_df['label_name'].value_counts(normalize=True).rename(\"share\")\n}, axis=1).sort_index().plot.bar(ax=ax)\nax.set_ylabel(\"proportion\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nClass balance (train vs. test)\n\n\n\n\n\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    df['len'] = df['sentence'].str.split().map(len)\n\ndesc = train_df.groupby('label_name')['len'].agg(\n    ['count','mean','std','median','min','max']\n).round(1)\ndesc\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmedian\nmin\nmax\n\n\nlabel_name\n\n\n\n\n\n\n\n\n\n\nnegative\n242\n24.3\n10.2\n22.0\n5\n54\n\n\nneutral\n1113\n21.0\n9.8\n19.0\n2\n81\n\n\npositive\n456\n24.8\n10.4\n23.0\n6\n57\n\n\n\n\n\nDistribution of headline length (tokens ≈ whitespace split)\n\n\n\nsns.kdeplot(data=train_df, x='len', hue='label_name', common_norm=False, fill=True)\nplt.xlabel(\"tokens per sentence\")\nplt.tight_layout(); plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "Introduction",
    "text": "Introduction\nThe introduction should:\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\n\nToday’s markets move nearly instantaneously to news headlines. A single Bloomberg article or tweet can push a mega-cap’s share price by hundreds of billions before most humans can finish reading it. For data-driven traders, headline sentiment is therefore a first-line signal that is used to track order flow, flag risk, and populate real-time dashboards. The question is which sentiment engine to trust: a lightweight rule-based lexicon written in 2014, or a 110 million- parameter transformer fine-tuned last night?"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "1.1 Research question",
    "text": "1.1 Research question\n\nOn the Financial PhraseBank corpus, does a fine‑tuned FinBERT model outperform the classic VADER lexicon—measured by macro‑F1 and overall accuracy—when classifying headlines as Positive, Neutral, or Negative?"
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "1.2 Why it matters",
    "text": "1.2 Why it matters\n\nData‑science lens  — Quantifies the return on GPU: is domain‑specific fine‑tuning worth the extra carbon, cost, and complexity compared to a zero‑training heuristic (Hutto and Gilbert 2014)?\nTrading lens  — Higher‑fidelity sentiment feeds translate directly into fewer false long/short triggers, cleaner event studies, and more accurate thematic basket construction. Even a five‑point F1 lift can save basis points on execution.\n\n\n\n\n\n\n\n\nComponent\nChoice\n\n\n\n\nDataset\nFinancial PhraseBank — “Sentences_AllAgree” split (4 ,840 labelled sentences).\n\n\nModel A\nVADER rule‑based scorer with 7 k‑term lexicon (Hutto and Gilbert 2014).\n\n\nModel B\nFinBERT (ProsusAI/finbert) fine‑tuned for three epochs on the same training split (Yang, Uy, and Huang 2020).\n\n\nMetrics\nMacro‑F1, overall accuracy, confusion matrix; CPU/GPU runtime.\n\n\n\nThe outcome will show whether heavy transformers deliver practical lift over a free, no‑GPU baseline in a realistic “small‑data” finance setting. \n\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nExample of writing including citing references:\nThis is an introduction to ….. regression, which is a non-parametric estimator that estimates the conditional expectation of two variables which is random. The goal of a kernel regression is to discover the non-linear relationship between two random variables. To discover the non-linear relationship, kernel estimator or kernel smoothing is the main method to estimate the curve for non-parametric statistics. In kernel estimator, weight function is known as kernel function (Efromovich 2008). Cite this paper (Bro and Smilde 2014). The GEE (Wang 2014). The PCA (Daffertshofer et al. 2004). Topology can be used in machine learning (Adams and Moy 2021)\nThis is my work and I want to add more work…"
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "Methods",
    "text": "Methods\n### Overview\n- Goal Compare a rule‑based and a transformer approach for classifying headline sentiment.\n- Dataset Financial PhraseBank (Sentences_AllAgree, N = 4 ,840).\n- Train/Test Split 80 % train : 20 % test (stratified).\n- Evaluation Metrics Macro‑F1, accuracy, confusion matrix; runtime footprint.\n\nDetail the models or algorithms used. ### Baseline Model – VADER\nAlgorithm 7 k‑term lexicon + 5 heuristic adjustments\n(negation, degree adverbs, all‑caps, punctuation emphasis, contrastive “but”).\n\nImplementation vaderSentiment 3.3.2 (Python).\n\nLabel Mapping\n\ncompound ≥ 0.05 is Positive\n\ncompound ≤ –0.05 is Negative\n\nelse Neutral\n\nComplexity O(N) pass over sentences; no training phase.\n\n\nFine‑Tuned Transformer – FinBERT\nCurrent proposed second model comparison below, with guidance from an LLM. Will change\n\nBase Model ProsusAI/finbert (110 M params, BERT‑Base).\n\nFine‑Tuning\n\nLoss cross‑entropy on 3‑class PhraseBank labels\n\nHyper‑params 3 epochs · batch 32 · lr 2 × 10⁻⁵ · max_len 64\n\nHardware single T4 GPU (~6 min).\n\n\nOutput Mapping soft‑max log‑probs -&gt; Positive/Neutral/Negative.\n\nRegularization early stopping on validation F1.\n\nObjective\n[ L = -{i=1}^{N}{c{+,0,-}} y_{ic};p_{}(c,|,h_i) ] - Justify your choices based on the problem and data."
  },
  {
    "objectID": "index.html#analysis-and-results",
    "href": "index.html#analysis-and-results",
    "title": "Writing a great story for data science projects - Summer 2025",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies.\n\nA study was conducted to determine how…\n{r, warning=FALSE, echo=T, message=FALSE} # loading packages  library(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs)\n```{r, warning=FALSE, echo=TRUE} # Load Data kable(head(murders))\nggplot1 = murders %&gt;% ggplot(mapping = aes(x=population/10^6, y=total))\nggplot1 + geom_point(aes(col=region), size = 4) + geom_text_repel(aes(label=abb)) + scale_x_log10() + scale_y_log10() + geom_smooth(formula = “y~x”, method=lm,se = F)+ xlab(“Populations in millions (log10 scale)”) + ylab(“Total number of murders (log10 scale)”) + ggtitle(“US Gun Murders in 2010”) + scale_color_discrete(name = “Region”)+ theme_bw()\n\n### Modeling and Results\n\n-   Explain your data preprocessing and cleaning steps.\n\n-   Present your key findings in a clear and concise manner.\n\n-   Use visuals to support your claims.\n\n-   **Tell a story about what the data reveals.**\n\n```{r}\n\n\n\nConclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "04_results.html",
    "href": "04_results.html",
    "title": "Page Title",
    "section": "",
    "text": "#| label: load_preds import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nvader = pd.read_csv(“data/processed/vader_preds.csv”) finb = pd.read_csv(“data/processed/finbert_preds.csv”)\ny_true = vader[‘label_name’] # same as finb[‘label_name’] labels = [“negative”,“neutral”,“positive”]\ndef metrics(df): f1 = f1_score(y_true, df[‘pred’], average=‘macro’) acc = accuracy_score(y_true, df[‘pred’]) return f1, acc\nm_vader = metrics(vader) m_finb = metrics(finb)\nsummary = pd.DataFrame({ “Model”: [“VADER”, “FinBERT (fine-tuned)”], “Macro‑F1”: [m_vader[0], m_finb[0]], “Accuracy”: [m_vader[1], m_finb[1]] }).round(3) summary\n#| label: cmaps #| fig-cap: “Confusion matrices (rows=true, cols=pred)” fig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True) for j,(name, df) in enumerate([(“VADER”, vader), (“FinBERT”, finb)]): cm = confusion_matrix(y_true, df[‘pred’], labels=labels) sns.heatmap(cm, annot=True, fmt=“d”, cmap=“Blues”, xticklabels=labels, yticklabels=labels, ax=axes[j]) axes[j].set_title(name) axes[j].set_xlabel(“Predicted”); axes[j].set_ylabel(“True”) plt.show()"
  }
]