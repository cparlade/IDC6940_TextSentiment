---
title: "Stock Market Headline News Sentiment"
subtitle: "One dataset • Two models • A fair comparison"
author: "Christian Parlade (Advisor: Dr. Cohen)"
date: "08-05-2025"
format:
  revealjs:
    slide-number: true
    transition: fade
    center: true
    theme: simple
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## 0) Agenda

- Problem & hypothesis

- Data (Financial PhraseBank)

- Baseline vs. Transformer

- Train/test & metrics

- Results -> where each model wins/loses

- Cost, limits, next steps

---

## 1) Problem

- **Question:** Do short **financial headlines** carry sentiment we can reliably classify?
- **Why care:** quick market mood checks, features for trading models, sanity-checks on dashboards.
- **Plan:** same dataset, same split to compare a **rule-based baseline** vs a **transformer**.
- **Hypothesis:** A finance-tuned transformer (FinBERT) will beat a rule-based baseline (VADER), especially on hedged or mixed-polarity headlines.

---

## 2) Data (Financial PhraseBank)

- ~4.8k single-sentence finance statements  
- Labels: **negative / neutral / positive**
- Pre-split **80/20 stratified** (same split for both models)

```{python}
from pathlib import Path
import pandas as pd
p = Path("data/processed")
ok = (p/"phrasebank_train.csv").exists() and (p/"phrasebank_test.csv").exists()
print("Train/Test CSVs present:", ok)
if ok:
    train = pd.read_csv(p/"phrasebank_train.csv")
    test  = pd.read_csv(p/"phrasebank_test.csv")
    print("Train size:", len(train), " Test size:", len(test))
```

---

## 3) EDA: class balance

```{python}
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
p = Path("data/processed")
if (p/"phrasebank_train.csv").exists() and (p/"phrasebank_test.csv").exists():
    train = pd.read_csv(p/"phrasebank_train.csv")
    test  = pd.read_csv(p/"phrasebank_test.csv")
    fig, ax = plt.subplots(figsize=(6,3.2))
    pd.concat({
        "train": train['label_name'].value_counts(normalize=True).rename("share"),
        "test":  test['label_name'].value_counts(normalize=True).rename("share")
    }, axis=1).sort_index().plot.bar(ax=ax)
    ax.set_ylabel("proportion"); ax.set_xlabel("label"); plt.tight_layout(); plt.show()
else:
    print("Waiting on phrasebank_train/test CSVs.")
```
- Neutral dominates -> strong prior; models must beat “predict neutral.”

---

## 4) EDA: length sanity check

```{python}
import seaborn as sns, matplotlib.pyplot as plt
if 'train' in globals():
    train['len'] = train['sentence'].str.split().map(len)
    ax = sns.kdeplot(data=train, x='len', hue='label_name', common_norm=False, fill=True)
    ax.set_xlabel("tokens per sentence"); plt.tight_layout(); plt.show()
else:
    print("Length plot skipped (no train CSV).")
```

---

## 5) Models 

- **VADER (baseline)**

    - Lexicon + rules (negation, intensity, punctuation, ALL-CAPS)

    - Fast & transparent; tends to over-predict neutral

- **FinBERT (fine-tuned)**

    - BERT pretrained on finance text

    - 3-epoch fine-tune; better at context and hedged language

--- 

## 6) Train/test process 

- Keep exact same 80/20 stratified split

- Metrics: Macro-F1 (primary) + Accuracy

- Same split, same preprocessing across models; results computed on the same test set.

---

## 7) Results

```{python}
import pandas as pd, numpy as np
from pathlib import Path
tbl = Path("results/summary_metrics.csv")
if tbl.exists():
    res = pd.read_csv(tbl)
    display(res.round(3))
else:
    # Compute quick table if preds exist
    vp, fp = Path("data/processed/vader_preds.csv"), Path("data/processed/finbert_preds.csv")
    from sklearn.metrics import f1_score, accuracy_score
    rows=[]
    if vp.exists():
        v = pd.read_csv(vp); y = v['label_name']
        rows.append(("VADER",
                     round(f1_score(y, v['pred'], average='macro'),3),
                     round(accuracy_score(y, v['pred']),3)))
    if fp.exists():
        f = pd.read_csv(fp); y = f['label_name']
        rows.append(("FinBERT (ft)",
                     round(f1_score(y, f['pred'], average='macro'),3),
                     round(accuracy_score(y, f['pred']),3)))
    if rows:
        res = pd.DataFrame(rows, columns=["Model","Macro-F1","Accuracy"])
        display(res)
    else:
        print("No summary_metrics.csv or prediction CSVs found yet.")
```

- FinBERT dominates: Macro-F1 0.925 vs VADER 0.487 | Accuracy 0.938 vs 0.575.

- Macro-F1 treats neg/neu/pos equally which means neutral-heavy models can’t coast.

- Lift is concentrated in neg/pos recovery from “neutral.”

- Lift comes from context + domain pretraining (hedged language, finance terms).

---

## 8) Confusion matrices

```{python}
import matplotlib.pyplot as plt, seaborn as sns
from sklearn.metrics import confusion_matrix
from pathlib import Path
labels = ["negative","neutral","positive"]
pv, pf = Path("figs/cm_vader.png"), Path("figs/cm_finbert.png")
vp, fp = Path("data/processed/vader_preds.csv"), Path("data/processed/finbert_preds.csv")

if pv.exists() and pf.exists():
    from PIL import Image
    fig, axes = plt.subplots(1,2, figsize=(9,3.6), constrained_layout=True)
    axes[0].imshow(Image.open(pv)); axes[0].axis("off"); axes[0].set_title("VADER")
    axes[1].imshow(Image.open(pf)); axes[1].axis("off"); axes[1].set_title("FinBERT")
    plt.show()
elif vp.exists():
    v = pd.read_csv(vp); y = v['label_name']
    fig, ax = plt.subplots(1,1, figsize=(4.5,3.6), constrained_layout=True)
    cm = confusion_matrix(y, v['pred'], labels=labels)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels, ax=ax)
    ax.set_title("VADER"); ax.set_xlabel("Predicted"); ax.set_ylabel("True"); plt.show()
else:
    print("Add figs/cm_vader.png & figs/cm_finbert.png or prediction CSVs.")
```

---

## Confusion Matrices continued

- **VADER:** large boundary errors - negatives and positives frequently land in neutral or even opposite classes.

- **FinBERT:** diagonals near 0.92-0.95 across all classes; residual mistakes are weak-polarity headlines.

- Biggest win: FinBERT disambiguates “mixed” headlines (e.g., “revenue up, margins compress”).

- FinBERT turns many VADER false neutrals into correct neg/pos, especially on hedged phrasing (‘despite’, ‘amid’).

---

## 9) Qualitative error analysis

- Mixed polarity: “Shares fall despite upbeat outlook.”

    - VADER: often neutral (cancels signals)

    - FinBERT: negative (weights market reaction)

- Hedging / finance jargon: “Guidance trimmed amid macro uncertainty.”

    - VADER: may drift neutral/positive (focus on “guidance”)

    - FinBERT: negative (learned ‘trimmed/uncertainty’ patterns)

- Forward-looking phrase: “Expects cost headwinds to ease.”

    - Both can wobble; FinBERT more likely positive given “ease”.


--- 

## 10) Cost & practicality

- **VADER**

    - Pros: transparent, instant on CPU, zero training.

    - Cons: brittle with hedging, negation scope, and domain phrasing.

- **FinBERT (fine-tuned)**

    - Pros: state-of-the-art accuracy, robust to phrasing.

    - Cons: GPU helpful for training; slightly heavier at inference (batch it).

- **Recommendation**

    - Use VADER for lightweight, glanceable dashboards.

    - Use FinBERT for reporting, alerts, or trading features where mistakes cost you.

---

## 11) Limits & next steps

- **Limits**

    - Short headlines only; English; dataset is older and domain drift likely.

    - No aspect sentiment (e.g., revenue vs. costs vs. guidance).

- **Next steps**

    - Domain adapt on recent 10-K MD&A / earnings call snippets.

    - Add aspect tags like “rating cut vs. price target raised.”

    - Calibrate thresholds for “uncertain/ambiguous” to avoid false confidence.

---

## 12) Takeaways

- Transformer lift is real: FinBERT turns many VADER “neutral”s into correct neg/pos, driving a big Macro-F1 gain.

- Metric choice matters: Use Macro-F1 as the KPI so the model can’t coast by predicting neutral in an imbalanced 3-class setup.

- When to choose which: VADER for speed/transparency and lightweight dashboards; FinBERT for decisioning/alerts where boundary flips cost you.

- Operationalization is straightforward: Train once; do batched CPU inference, cache predictions, and monitor class drift.

- Next step with impact: Domain-adapt on recent filings/calls and add aspect sentiment (revenue/costs/guidance) to make signals actionable.

---

## References

- Hutto & Gilbert (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis.
- Yang, Uy, & Huang (2020). FinBERT: Pre-trained Language Model for Financial Communications (arXiv:2006.08097).
- Loughran & McDonald (2011). When Is a Liability Not a Liability? Journal of Finance (finance sentiment lexicon context).
