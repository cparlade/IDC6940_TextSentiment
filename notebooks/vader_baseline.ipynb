{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2338c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time, pathlib, numpy as np, pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373dfc46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = pathlib.Path(\"data/processed\")\n",
    "test_df = pd.read_csv(DATA_DIR/\"phrasebank_test.csv\")\n",
    "\n",
    "# Map VADER compound to 3 classes\n",
    "def vader_label(comp):\n",
    "    if comp >= 0.05:   return \"positive\"\n",
    "    if comp <= -0.05:  return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "an = SentimentIntensityAnalyzer()\n",
    "t0 = time.perf_counter()\n",
    "scores = test_df['sentence'].apply(lambda s: an.polarity_scores(s)['compound'])\n",
    "preds = scores.map(vader_label)\n",
    "runtime_s = time.perf_counter() - t0\n",
    "\n",
    "out = test_df.copy()\n",
    "out['pred'] = preds\n",
    "out.to_csv(DATA_DIR/\"vader_preds.csv\", index=False)\n",
    "\n",
    "y_true = test_df['label_name']\n",
    "macro_f1 = f1_score(y_true, preds, average='macro')\n",
    "acc = accuracy_score(y_true, preds)\n",
    "print(f\"VADER  macro-F1={macro_f1:.3f}  acc={acc:.3f}  runtime={runtime_s:.2f}s\")\n",
    "print(confusion_matrix(y_true, preds, labels=[\"negative\",\"neutral\",\"positive\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
